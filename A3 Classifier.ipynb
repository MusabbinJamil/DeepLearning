{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89564,"databundleVersionId":10327706,"sourceType":"competition"},{"sourceId":214466,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":182823,"modelId":205032}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:29:20.633745Z","iopub.execute_input":"2024-12-30T15:29:20.634126Z","iopub.status.idle":"2024-12-30T15:29:50.726356Z","shell.execute_reply.started":"2024-12-30T15:29:20.634100Z","shell.execute_reply":"2024-12-30T15:29:50.725318Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/resnet-weights/pytorch/default/1/resnet18-f37072fd.pth\n/kaggle/input/dog-vs-cat-vs-bird/dataset/sample_submission.csv\n/kaggle/input/dog-vs-cat-vs-bird/dataset/test/img_1963.png\n/kaggle/input/dog-vs-cat-vs-bird/dataset/train/cat_9146.png\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Baseline Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:29:50.727890Z","iopub.execute_input":"2024-12-30T15:29:50.728489Z","iopub.status.idle":"2024-12-30T15:29:54.089193Z","shell.execute_reply.started":"2024-12-30T15:29:50.728451Z","shell.execute_reply":"2024-12-30T15:29:54.088192Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:29:54.091270Z","iopub.execute_input":"2024-12-30T15:29:54.091774Z","iopub.status.idle":"2024-12-30T15:29:54.095865Z","shell.execute_reply.started":"2024-12-30T15:29:54.091745Z","shell.execute_reply":"2024-12-30T15:29:54.094975Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:29:54.097516Z","iopub.execute_input":"2024-12-30T15:29:54.097810Z","iopub.status.idle":"2024-12-30T15:29:54.596546Z","shell.execute_reply.started":"2024-12-30T15:29:54.097786Z","shell.execute_reply":"2024-12-30T15:29:54.595329Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_csv = pd.read_csv('/kaggle/input/dog-vs-cat-vs-bird/dataset/train.csv')\nprint(train_csv['label'].unique())  # Ensure this outputs only [0, 1, 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:29:54.597720Z","iopub.execute_input":"2024-12-30T15:29:54.598325Z","iopub.status.idle":"2024-12-30T15:29:54.644803Z","shell.execute_reply.started":"2024-12-30T15:29:54.598293Z","shell.execute_reply":"2024-12-30T15:29:54.643811Z"}},"outputs":[{"name":"stdout","text":"['cat' 'dog' 'bird']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"images, labels = next(iter(train_loader))\noutputs = model(images.to(device))\nprint(f\"Model output shape: {outputs.shape}\")  # Should be [batch_size, 3]\nprint(f\"Labels: {labels}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:08:46.223510Z","iopub.execute_input":"2024-12-27T12:08:46.223905Z","iopub.status.idle":"2024-12-27T12:08:46.797497Z","shell.execute_reply.started":"2024-12-27T12:08:46.223869Z","shell.execute_reply":"2024-12-27T12:08:46.796293Z"}},"outputs":[{"name":"stdout","text":"Model output shape: torch.Size([64, 3])\nLabels: ('cat', 'cat', 'cat', 'bird', 'bird', 'cat', 'cat', 'bird', 'bird', 'cat', 'cat', 'bird', 'cat', 'dog', 'bird', 'cat', 'cat', 'dog', 'bird', 'cat', 'bird', 'cat', 'bird', 'cat', 'cat', 'cat', 'bird', 'cat', 'dog', 'bird', 'cat', 'dog', 'bird', 'bird', 'bird', 'bird', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'bird', 'bird', 'bird', 'bird', 'dog', 'dog', 'cat', 'bird', 'dog', 'bird', 'cat', 'bird', 'bird', 'dog', 'bird', 'cat', 'cat', 'dog', 'cat', 'dog', 'bird')\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        # Read CSV and store the image data and labels\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        # Initialize the LabelEncoder to convert categorical labels to numeric\n        self.label_encoder = LabelEncoder()\n        # Fit the encoder on all unique labels\n        self.data.iloc[:, 1] = self.label_encoder.fit_transform(self.data.iloc[:, 1])\n\n    def __len__(self):\n        # Return the total number of samples\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Get the image path and label\n        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])  # Image filename\n        image = Image.open(img_name).convert('RGB')  # Open and convert to RGB\n        label = self.data.iloc[idx, 1]  # Categorical label (already encoded to integer)\n\n        # Apply transformation if any (e.g., resize, normalize)\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n    def get_label_encoder(self):\n        # Return the fitted label encoder\n        return self.label_encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:00.753266Z","iopub.execute_input":"2024-12-30T15:30:00.753591Z","iopub.status.idle":"2024-12-30T15:30:00.760631Z","shell.execute_reply.started":"2024-12-30T15:30:00.753566Z","shell.execute_reply":"2024-12-30T15:30:00.759520Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"img_path = '/kaggle/input/dog-vs-cat-vs-bird/dataset/train/cat_9146.png'\nimage = Image.open(img_path)\nprint(f\"Original Image Mode: {image.mode}\")  # Should show 'RGBA'\n\nimage = image.convert('RGB')\nprint(f\"Converted Image Mode: {image.mode}\")  # Should show 'RGB'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:31:41.665075Z","iopub.execute_input":"2024-12-27T12:31:41.665476Z","iopub.status.idle":"2024-12-27T12:31:41.674695Z","shell.execute_reply.started":"2024-12-27T12:31:41.665445Z","shell.execute_reply":"2024-12-27T12:31:41.673692Z"}},"outputs":[{"name":"stdout","text":"Original Image Mode: RGBA\nConverted Image Mode: RGB\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:11.177613Z","iopub.execute_input":"2024-12-30T15:30:11.178007Z","iopub.status.idle":"2024-12-30T15:30:11.183985Z","shell.execute_reply.started":"2024-12-30T15:30:11.177978Z","shell.execute_reply":"2024-12-30T15:30:11.182721Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = CustomDataset(\n    csv_file='/kaggle/input/dog-vs-cat-vs-bird/dataset/train.csv',\n    root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/train',\n    transform=transform\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:12.993947Z","iopub.execute_input":"2024-12-30T15:30:12.994325Z","iopub.status.idle":"2024-12-30T15:30:13.019723Z","shell.execute_reply.started":"2024-12-30T15:30:12.994295Z","shell.execute_reply":"2024-12-30T15:30:13.018238Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"label_encoder = train_dataset.get_label_encoder()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:17.598017Z","iopub.execute_input":"2024-12-30T15:30:17.598359Z","iopub.status.idle":"2024-12-30T15:30:17.603283Z","shell.execute_reply.started":"2024-12-30T15:30:17.598334Z","shell.execute_reply":"2024-12-30T15:30:17.602074Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:21.080772Z","iopub.execute_input":"2024-12-30T15:30:21.081168Z","iopub.status.idle":"2024-12-30T15:30:21.086220Z","shell.execute_reply.started":"2024-12-30T15:30:21.081138Z","shell.execute_reply":"2024-12-30T15:30:21.085001Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"images, labels = next(iter(train_loader))\nprint(f\"Image batch shape: {images.shape}\")  # Should be [batch_size, 3, height, width]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:32:01.938205Z","iopub.execute_input":"2024-12-27T12:32:01.938582Z","iopub.status.idle":"2024-12-27T12:32:02.115388Z","shell.execute_reply.started":"2024-12-27T12:32:01.938553Z","shell.execute_reply":"2024-12-27T12:32:02.114262Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([64, 3, 32, 32])\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Linear(128, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:09:28.607102Z","iopub.execute_input":"2024-12-27T12:09:28.607502Z","iopub.status.idle":"2024-12-27T12:09:28.614675Z","shell.execute_reply.started":"2024-12-27T12:09:28.607474Z","shell.execute_reply":"2024-12-27T12:09:28.613269Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:26:38.345877Z","iopub.execute_input":"2024-12-30T14:26:38.346190Z","iopub.status.idle":"2024-12-30T14:26:38.364437Z","shell.execute_reply.started":"2024-12-30T14:26:38.346166Z","shell.execute_reply":"2024-12-30T14:26:38.362958Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-5c3b0abd5a40>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SimpleCNN' is not defined"],"ename":"NameError","evalue":"name 'SimpleCNN' is not defined","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"def train_model(model, train_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        train_loss, correct = 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            correct += (outputs.argmax(1) == labels).sum().item()\n\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f}, Accuracy: {correct/len(train_dataset):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:09:38.169974Z","iopub.execute_input":"2024-12-27T12:09:38.170396Z","iopub.status.idle":"2024-12-27T12:09:38.176451Z","shell.execute_reply.started":"2024-12-27T12:09:38.170362Z","shell.execute_reply":"2024-12-27T12:09:38.175329Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"train_model(model, train_loader, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:09:42.099832Z","iopub.execute_input":"2024-12-27T12:09:42.100212Z","iopub.status.idle":"2024-12-27T12:15:39.604519Z","shell.execute_reply.started":"2024-12-27T12:09:42.100182Z","shell.execute_reply":"2024-12-27T12:15:39.603309Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 | Loss: 0.9269, Accuracy: 0.5503\nEpoch 2/10 | Loss: 0.7895, Accuracy: 0.6435\nEpoch 3/10 | Loss: 0.7213, Accuracy: 0.6809\nEpoch 4/10 | Loss: 0.6542, Accuracy: 0.7212\nEpoch 5/10 | Loss: 0.5996, Accuracy: 0.7479\nEpoch 6/10 | Loss: 0.5413, Accuracy: 0.7727\nEpoch 7/10 | Loss: 0.4826, Accuracy: 0.7993\nEpoch 8/10 | Loss: 0.4106, Accuracy: 0.8364\nEpoch 9/10 | Loss: 0.3485, Accuracy: 0.8643\nEpoch 10/10 | Loss: 0.2749, Accuracy: 0.8960\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.image_names = sorted(os.listdir(root_dir))  # List of image filenames in the test directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_names[idx])  # Path to the image\n        image = Image.open(img_name).convert('RGB')  # Convert to RGB to handle RGBA images\n\n        if self.transform:\n            image = self.transform(image)  # Apply transformations if any\n\n        return image, self.image_names[idx]  # Return image and filename for tracking","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:21:04.940443Z","iopub.execute_input":"2024-12-30T11:21:04.940841Z","iopub.status.idle":"2024-12-30T11:21:04.947402Z","shell.execute_reply.started":"2024-12-30T11:21:04.940797Z","shell.execute_reply":"2024-12-30T11:21:04.946122Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"test_dataset = TestDataset(root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/test', transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:21:08.002501Z","iopub.execute_input":"2024-12-30T11:21:08.002833Z","iopub.status.idle":"2024-12-30T11:21:08.070374Z","shell.execute_reply.started":"2024-12-30T11:21:08.002797Z","shell.execute_reply":"2024-12-30T11:21:08.069005Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model.eval()\npredictions = []\nfilenames = []\n\nwith torch.no_grad():\n    for images, file_names in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(1).cpu().numpy()  # Get class predictions (indices)\n        predictions.extend(preds)\n        filenames.extend(file_names)  # Keep track of filenames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:27:00.705151Z","iopub.execute_input":"2024-12-27T12:27:00.705523Z","iopub.status.idle":"2024-12-27T12:27:33.944869Z","shell.execute_reply.started":"2024-12-27T12:27:00.705495Z","shell.execute_reply":"2024-12-27T12:27:33.943848Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"predicted_labels = label_encoder.inverse_transform(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:33:14.227305Z","iopub.execute_input":"2024-12-27T12:33:14.227630Z","iopub.status.idle":"2024-12-27T12:33:14.235544Z","shell.execute_reply.started":"2024-12-27T12:33:14.227605Z","shell.execute_reply":"2024-12-27T12:33:14.234359Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"submission_df = pd.DataFrame({\n    'filename': filenames,\n    'label': predicted_labels\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:43:33.366604Z","iopub.execute_input":"2024-12-27T12:43:33.366971Z","iopub.status.idle":"2024-12-27T12:43:33.372233Z","shell.execute_reply.started":"2024-12-27T12:43:33.366940Z","shell.execute_reply":"2024-12-27T12:43:33.371082Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:43:36.268903Z","iopub.execute_input":"2024-12-27T12:43:36.269335Z","iopub.status.idle":"2024-12-27T12:43:36.278179Z","shell.execute_reply.started":"2024-12-27T12:43:36.269301Z","shell.execute_reply":"2024-12-27T12:43:36.277264Z"}},"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"       filename label\n0     img_0.png   dog\n1     img_1.png   dog\n2    img_10.png   cat\n3   img_100.png   cat\n4  img_1000.png   cat","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img_0.png</td>\n      <td>dog</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img_1.png</td>\n      <td>dog</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img_10.png</td>\n      <td>cat</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img_100.png</td>\n      <td>cat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img_1000.png</td>\n      <td>cat</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:43:41.058551Z","iopub.execute_input":"2024-12-27T12:43:41.058899Z","iopub.status.idle":"2024-12-27T12:43:41.067732Z","shell.execute_reply.started":"2024-12-27T12:43:41.058872Z","shell.execute_reply":"2024-12-27T12:43:41.066726Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/working/submission.csv')\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:43:43.645985Z","iopub.execute_input":"2024-12-27T12:43:43.646435Z","iopub.status.idle":"2024-12-27T12:43:43.656983Z","shell.execute_reply.started":"2024-12-27T12:43:43.646401Z","shell.execute_reply":"2024-12-27T12:43:43.655943Z"}},"outputs":[{"name":"stdout","text":"       filename label\n0     img_0.png   dog\n1     img_1.png   dog\n2    img_10.png   cat\n3   img_100.png   cat\n4  img_1000.png   cat\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"sample_submission_df = pd.read_csv('/kaggle/input/dog-vs-cat-vs-bird/dataset/sample_submission.csv')\nprint(sample_submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T12:43:46.595788Z","iopub.execute_input":"2024-12-27T12:43:46.596154Z","iopub.status.idle":"2024-12-27T12:43:46.608352Z","shell.execute_reply.started":"2024-12-27T12:43:46.596122Z","shell.execute_reply":"2024-12-27T12:43:46.607232Z"}},"outputs":[{"name":"stdout","text":"    filename label\n0  img_0.png   cat\n1  img_1.png   dog\n2  img_2.png  bird\n3  img_3.png   cat\n4  img_4.png   dog\n","output_type":"stream"}],"execution_count":116},{"cell_type":"markdown","source":"## Optimization","metadata":{}},{"cell_type":"markdown","source":"### Dropout","metadata":{}},{"cell_type":"code","source":"class SimpleCNNWithDropout(nn.Module):\n    def __init__(self):\n        super(SimpleCNNWithDropout, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),  # Dropout with 25% rate\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),  # Dropout with 50% rate\n            nn.Linear(128, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel = SimpleCNNWithDropout().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Rate Scheduler","metadata":{}},{"cell_type":"code","source":"scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR by 10% every 5 epochs\n\ndef train_model_with_scheduler(model, train_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        train_loss, correct = 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            correct += (outputs.argmax(1) == labels).sum().item()\n\n        scheduler.step()  # Adjust learning rate\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f}, Accuracy: {correct/len(train_dataset):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model_with_scheduler(model, train_loader, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"transform_with_augmentation = transforms.Compose([\n    transforms.RandomResizedCrop(32),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = CustomDataset(\n    csv_file='/kaggle/input/dog-vs-cat-vs-bird/dataset/train.csv',\n    root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/train',\n    transform=transform_with_augmentation\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport os\n\n# New model definition with Dropout\nclass SimpleCNNWithDropout(nn.Module):\n    def __init__(self):\n        super(SimpleCNNWithDropout, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),  # Dropout with 25% rate\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),  # Dropout with 50% rate\n            nn.Linear(128, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.fc(x)\n        return x\n\n# Data transformation with augmentation\ntransform_with_augmentation = transforms.Compose([\n    transforms.RandomResizedCrop(32),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Update train dataset with augmented transformations\ntrain_dataset = CustomDataset(\n    csv_file='/kaggle/input/dog-vs-cat-vs-bird/dataset/train.csv',\n    root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/train',\n    transform=transform_with_augmentation\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model, optimizer, and criterion\nmodel = SimpleCNNWithDropout().to(device)  # Use the new model with dropout\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:12:02.133844Z","iopub.execute_input":"2024-12-30T11:12:02.134270Z","iopub.status.idle":"2024-12-30T11:12:02.164074Z","shell.execute_reply.started":"2024-12-30T11:12:02.134235Z","shell.execute_reply":"2024-12-30T11:12:02.162865Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Set up the learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR by 10% every 5 epochs\n\n# Training function with scheduler\ndef train_model_with_scheduler(model, train_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        train_loss, correct = 0, 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            correct += (outputs.argmax(1) == labels).sum().item()\n\n        scheduler.step()  # Adjust learning rate\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f}, Accuracy: {correct/len(train_dataset):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:12:23.904314Z","iopub.execute_input":"2024-12-30T11:12:23.904640Z","iopub.status.idle":"2024-12-30T11:12:23.912048Z","shell.execute_reply.started":"2024-12-30T11:12:23.904617Z","shell.execute_reply":"2024-12-30T11:12:23.911026Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Train the model\ntrain_model_with_scheduler(model, train_loader, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:12:28.142557Z","iopub.execute_input":"2024-12-30T11:12:28.142963Z","iopub.status.idle":"2024-12-30T11:20:37.488361Z","shell.execute_reply.started":"2024-12-30T11:12:28.142891Z","shell.execute_reply":"2024-12-30T11:20:37.487232Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 | Loss: 1.0506, Accuracy: 0.4457\nEpoch 2/10 | Loss: 1.0130, Accuracy: 0.4773\nEpoch 3/10 | Loss: 0.9934, Accuracy: 0.5028\nEpoch 4/10 | Loss: 0.9848, Accuracy: 0.5104\nEpoch 5/10 | Loss: 0.9853, Accuracy: 0.5097\nEpoch 6/10 | Loss: 0.9616, Accuracy: 0.5282\nEpoch 7/10 | Loss: 0.9608, Accuracy: 0.5269\nEpoch 8/10 | Loss: 0.9482, Accuracy: 0.5401\nEpoch 9/10 | Loss: 0.9526, Accuracy: 0.5344\nEpoch 10/10 | Loss: 0.9456, Accuracy: 0.5407\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Same normalization as training\n])\n\n# Create the test dataset and loader\ntest_dataset = TestDataset(\n    root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/test',\n    transform=transform\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:21:46.600739Z","iopub.execute_input":"2024-12-30T11:21:46.601244Z","iopub.status.idle":"2024-12-30T11:21:46.611606Z","shell.execute_reply.started":"2024-12-30T11:21:46.601210Z","shell.execute_reply":"2024-12-30T11:21:46.610525Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model.eval()  # Set the model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:21:49.207602Z","iopub.execute_input":"2024-12-30T11:21:49.208022Z","iopub.status.idle":"2024-12-30T11:21:49.215265Z","shell.execute_reply.started":"2024-12-30T11:21:49.207986Z","shell.execute_reply":"2024-12-30T11:21:49.214230Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"SimpleCNNWithDropout(\n  (conv): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Dropout(p=0.25, inplace=False)\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=4096, out_features=128, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=128, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"predictions = []\nfilenames = []\n\n# Make predictions\nwith torch.no_grad():  # No need to compute gradients for inference\n    for images, file_names in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(1).cpu().numpy()  # Get the predicted class labels (as integers)\n        predictions.extend(preds)\n        filenames.extend(file_names)  # Keep track of the filenames for the results\n\n# Convert the predictions to actual labels using the label encoder (if necessary)\npredicted_labels = label_encoder.inverse_transform(predictions)\n\n# Create a DataFrame to store the results\nsubmission_df = pd.DataFrame({\n    'filename': filenames,\n    'label': predicted_labels\n})\n\n# Optionally, save the predictions to a CSV file for submission\nsubmission_df.to_csv('/kaggle/working/submission_optimized.csv', index=False)\n\n# Check the first few rows of the submission DataFrame\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:24:26.101247Z","iopub.execute_input":"2024-12-30T11:24:26.101583Z","iopub.status.idle":"2024-12-30T11:24:33.269429Z","shell.execute_reply.started":"2024-12-30T11:24:26.101555Z","shell.execute_reply":"2024-12-30T11:24:33.268351Z"}},"outputs":[{"name":"stdout","text":"       filename label\n0     img_0.png   cat\n1     img_1.png   dog\n2    img_10.png   cat\n3   img_100.png   cat\n4  img_1000.png  bird\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## ResNet","metadata":{}},{"cell_type":"code","source":"# Step 1: Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom PIL import Image\nimport os\nimport pandas as pd\n\n# Step 2: Define the Custom Dataset for training\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Initialize the LabelEncoder if necessary\n        self.label_encoder = LabelEncoder()\n        self.data.iloc[:, 1] = self.label_encoder.fit_transform(self.data.iloc[:, 1])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n        image = Image.open(img_name).convert('RGB')\n        label = self.data.iloc[idx, 1]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Step 3: Define transformations for training and testing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to match the input size for ResNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Step 4: Load your train dataset\ntrain_dataset = CustomDataset(\n    csv_file='/kaggle/input/dog-vs-cat-vs-bird/dataset/train.csv',\n    root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/train',\n    transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:27.783870Z","iopub.execute_input":"2024-12-30T15:30:27.784238Z","iopub.status.idle":"2024-12-30T15:30:27.813990Z","shell.execute_reply.started":"2024-12-30T15:30:27.784211Z","shell.execute_reply":"2024-12-30T15:30:27.812568Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Step 5: Load pre-trained ResNet-18 model\nresnet = models.resnet18(pretrained=False)  # Initialize without downloading\nweights_path = \"/kaggle/input/resnet-weights/pytorch/default/1/resnet18-f37072fd.pth\"  # Provide path to the manually downloaded weights file\nresnet.load_state_dict(torch.load(weights_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:32.017954Z","iopub.execute_input":"2024-12-30T15:30:32.018296Z","iopub.status.idle":"2024-12-30T15:30:32.912156Z","shell.execute_reply.started":"2024-12-30T15:30:32.018270Z","shell.execute_reply":"2024-12-30T15:30:32.911154Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-13-c1a7139053cd>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  resnet.load_state_dict(torch.load(weights_path))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Modify the final fully connected layer to match your number of classes (3)\nresnet.fc = nn.Linear(resnet.fc.in_features, 3)\n\n# Freeze all layers except the final fully connected layer\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Only the parameters of the final fully connected layer will require gradients\nfor param in resnet.fc.parameters():\n    param.requires_grad = True\n\n# Step 6: Move the model to the appropriate device (GPU or CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nresnet = resnet.to(device)\n\n# Step 7: Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:36.160947Z","iopub.execute_input":"2024-12-30T15:30:36.161322Z","iopub.status.idle":"2024-12-30T15:30:36.178898Z","shell.execute_reply.started":"2024-12-30T15:30:36.161290Z","shell.execute_reply":"2024-12-30T15:30:36.177343Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Step 8: Train the model\ndef train_model(model, train_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        running_loss, correct = 0.0, 0\n        total = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n        # Print training results\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:39.768070Z","iopub.execute_input":"2024-12-30T15:30:39.768422Z","iopub.status.idle":"2024-12-30T15:30:39.775093Z","shell.execute_reply.started":"2024-12-30T15:30:39.768393Z","shell.execute_reply":"2024-12-30T15:30:39.773830Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Train the model\ntrain_model(resnet, train_loader, epochs=10)\n\n# Step 9: Save the trained model\ntorch.save(resnet.state_dict(), 'resnet_finetuned.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:30:42.928769Z","iopub.execute_input":"2024-12-30T15:30:42.929175Z","iopub.status.idle":"2024-12-30T15:45:12.891108Z","shell.execute_reply.started":"2024-12-30T15:30:42.929144Z","shell.execute_reply":"2024-12-30T15:45:12.889729Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.6402, Accuracy: 0.7321\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-e97be26cb20c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Step 9: Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnet_finetuned.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-0d4903830e97>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    165\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                             return_indices=self.return_indices)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# Step 10: Testing the model (for images without CSV file)\nclass TestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.image_names = sorted(os.listdir(root_dir))  # List of image filenames in the test directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_names[idx])\n        image = Image.open(img_name).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, self.image_names[idx]\n\n# Load the test dataset (images only, no labels)\ntest_dataset = TestDataset(root_dir='/kaggle/input/dog-vs-cat-vs-bird/dataset/test', transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Set model to evaluation mode for testing\nresnet.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:45:40.988726Z","iopub.execute_input":"2024-12-30T15:45:40.989145Z","iopub.status.idle":"2024-12-30T15:45:41.004449Z","shell.execute_reply.started":"2024-12-30T15:45:40.989111Z","shell.execute_reply":"2024-12-30T15:45:41.003396Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"predictions = []\nfilenames = []\n\n# Make predictions\nwith torch.no_grad():\n    for images, file_names in test_loader:\n        images = images.to(device)\n        outputs = resnet(images)\n        preds = outputs.argmax(1).cpu().numpy()  # Get predicted class labels\n        predictions.extend(preds)\n        filenames.extend(file_names)\n\n# Convert predictions to class labels\nlabel_encoder = train_dataset.label_encoder\npredicted_labels = label_encoder.inverse_transform(predictions)\n\n# Save the results to a CSV file\nsubmission_df = pd.DataFrame({\n    'filename': filenames,\n    'label': predicted_labels\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:45:46.682638Z","iopub.execute_input":"2024-12-30T15:45:46.683074Z","iopub.status.idle":"2024-12-30T15:48:54.481395Z","shell.execute_reply.started":"2024-12-30T15:45:46.683040Z","shell.execute_reply":"2024-12-30T15:48:54.480456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission_resnet.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:53:50.309518Z","iopub.execute_input":"2024-12-30T15:53:50.309981Z","iopub.status.idle":"2024-12-30T15:53:50.319949Z","shell.execute_reply.started":"2024-12-30T15:53:50.309938Z","shell.execute_reply":"2024-12-30T15:53:50.318778Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Fine Tuning Transfer Learning","metadata":{}},{"cell_type":"code","source":"# Unfreeze selected layers\nfor name, param in resnet.named_parameters():\n    if \"layer4\" in name:  # Unfreeze the last residual block\n        param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:54:05.051450Z","iopub.execute_input":"2024-12-30T15:54:05.051890Z","iopub.status.idle":"2024-12-30T15:54:05.058189Z","shell.execute_reply.started":"2024-12-30T15:54:05.051857Z","shell.execute_reply":"2024-12-30T15:54:05.056038Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.0001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:54:07.326428Z","iopub.execute_input":"2024-12-30T15:54:07.326763Z","iopub.status.idle":"2024-12-30T15:54:07.332118Z","shell.execute_reply.started":"2024-12-30T15:54:07.326737Z","shell.execute_reply":"2024-12-30T15:54:07.330956Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"resnet.eval()\npredictions, filenames = [], []\n\nwith torch.no_grad():\n    for images, file_names in test_loader:\n        images = images.to(device)\n        outputs = resnet(images)\n        preds = outputs.argmax(1).cpu().numpy()\n        predictions.extend(preds)\n        filenames.extend(file_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:54:09.530308Z","iopub.execute_input":"2024-12-30T15:54:09.530633Z","iopub.status.idle":"2024-12-30T15:57:00.521495Z","shell.execute_reply.started":"2024-12-30T15:54:09.530607Z","shell.execute_reply":"2024-12-30T15:57:00.520245Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"predicted_labels = label_encoder.inverse_transform(predictions)\nsubmission_df = pd.DataFrame({'filename': filenames, 'label': predicted_labels})\nsubmission_df.to_csv('/kaggle/working/submission_resnet_optimization.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T15:57:43.621632Z","iopub.execute_input":"2024-12-30T15:57:43.622023Z","iopub.status.idle":"2024-12-30T15:57:43.634246Z","shell.execute_reply.started":"2024-12-30T15:57:43.621993Z","shell.execute_reply":"2024-12-30T15:57:43.633075Z"}},"outputs":[],"execution_count":24}]}