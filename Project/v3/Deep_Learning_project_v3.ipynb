{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nSlpn_x8SFzC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset"
      ],
      "metadata": {
        "id": "hisszrXybS1k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "frEiB4lDbRp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "bGLorfrtVxD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "weight_decay = 1e-4\n",
        "batch_size = 128\n",
        "momentum = 0.9\n",
        "lambda_reg = 0.01"
      ],
      "metadata": {
        "id": "c8mZtKblS_5P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def siamese_l1_distance(vectors):\n",
        "    (h1, h2) = vectors\n",
        "    return tf.reduce_sum(tf.abs(h1 - h2), axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "DldpDpHrTCKe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_decay_schedule(epoch):\n",
        "    return learning_rate * (0.99 ** epoch)"
      ],
      "metadata": {
        "id": "AFbHpFemTEr9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_csn(input_shape):\n",
        "    def convolutional_block(input_shape=None):\n",
        "        layers_list = [\n",
        "            layers.Conv2D(64, (3, 3), strides=1, padding='same',\n",
        "                          kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1e-2),\n",
        "                          bias_initializer=tf.keras.initializers.RandomNormal(mean=0.5, stddev=1e-2),\n",
        "                          kernel_regularizer=regularizers.l2(lambda_reg)),\n",
        "            layers.ReLU(),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2), strides=2)\n",
        "        ]\n",
        "        if input_shape:\n",
        "            layers_list.insert(0, layers.Input(shape=input_shape))\n",
        "        return models.Sequential(layers_list)\n",
        "\n",
        "    # Define inputs for twin networks\n",
        "    input_a = layers.Input(shape=input_shape)\n",
        "    input_b = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv_net = models.Sequential([\n",
        "        convolutional_block(input_shape),\n",
        "        convolutional_block(),\n",
        "        convolutional_block()\n",
        "    ])\n",
        "\n",
        "    # Shared convolutional layers\n",
        "    features_a = conv_net(input_a)\n",
        "    features_b = conv_net(input_b)\n",
        "\n",
        "    # Flatten features\n",
        "    features_a = layers.Flatten()(features_a)\n",
        "    features_b = layers.Flatten()(features_b)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc = layers.Dense(128, activation='sigmoid',\n",
        "                      kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=2e-1),\n",
        "                      kernel_regularizer=regularizers.l2(lambda_reg))\n",
        "    fc_a = fc(features_a)\n",
        "    fc_b = fc(features_b)\n",
        "\n",
        "    # Compute L1 distance\n",
        "    l1_distance = layers.Lambda(siamese_l1_distance)([fc_a, fc_b])\n",
        "\n",
        "    # Final sigmoid layer for similarity\n",
        "    output = layers.Dense(1, activation='sigmoid')(l1_distance)\n",
        "\n",
        "    # CSN Model\n",
        "    model = models.Model(inputs=[input_a, input_b], outputs=output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "r_xEyT3BTGL8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function\n",
        "@tf.function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    ce_loss = -y_true * tf.math.log(y_pred + 1e-8) - (1 - y_true) * tf.math.log(1 - y_pred + 1e-8)\n",
        "    ce_loss = tf.reduce_mean(ce_loss)\n",
        "    reg_loss = tf.add_n([tf.nn.l2_loss(var) for var in model.trainable_variables])\n",
        "    return ce_loss + lambda_reg * reg_loss"
      ],
      "metadata": {
        "id": "nPgydHCbTIMH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)"
      ],
      "metadata": {
        "id": "OCrSm2MhTJQg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "input_shape = (105, 105, 1)\n",
        "model = build_csn(input_shape)\n",
        "model.compile(optimizer=optimizer, loss=custom_loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ioj-6qrlTKYm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataset, val_dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Training loop\n",
        "        for step, (pair_batch, label_batch) in enumerate(zip(train_pairs, train_labels)):\n",
        "            input_a_batch = pair_batch[:, 0]  # First image in the pair\n",
        "            input_b_batch = pair_batch[:, 1]  # Second image in the pair\n",
        "\n",
        "            # Ensure both images are batches of the same size\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model([input_a_batch, input_b_batch], training=True)  # Pass both inputs\n",
        "                loss = custom_loss(label_batch, y_pred)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(f\"Step {step}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss, val_acc = model.evaluate(val_dataset)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "MMMLXaYATL4l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory Module (Example Implementation)\n",
        "def memory_module(query, memory_keys, memory_values):\n",
        "    similarities = tf.matmul(query, tf.transpose(memory_keys))\n",
        "    nearest_idx = tf.argmax(similarities, axis=1)\n",
        "    return tf.gather(memory_values, nearest_idx)"
      ],
      "metadata": {
        "id": "OmICco1hTNrE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data and pre processing"
      ],
      "metadata": {
        "id": "sQNrqJyLV0kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './omniglot_data'"
      ],
      "metadata": {
        "id": "bXMc0c25Uurj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "AYhe2ybqVM_b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Omniglot(\n",
        "    root=data_path,\n",
        "    background=True,  # Load the background set\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")"
      ],
      "metadata": {
        "id": "F5vapfQLVM87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5302b240-f48c-414b-c8cc-c87c38051e82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.Omniglot(\n",
        "    root=data_path,\n",
        "    background=False,  # Load the evaluation set\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnPLsM8qWeGz",
        "outputId": "8c5ce9fe-8c94-4a80-bbe4-8f07a13fb934"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define indices for splitting the train set into training and validation\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tXnyEBXgbWx7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset = Subset(train_dataset, train_indices)\n",
        "val_subset = Subset(train_dataset, val_indices)"
      ],
      "metadata": {
        "id": "1rnhRB7HbX7L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "kazfuCjmbPMk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training Samples: {len(train_subset)}, Validation Samples: {len(val_subset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU7d0dY9baZK",
        "outputId": "a175e1fe-3d9d-4dbf-ef82-34f5a4f9b321"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Samples: 15424, Validation Samples: 3856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs(dataset, num_pairs, image_size=(105, 105)):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    # Randomly sample pairs from the dataset\n",
        "    for _ in range(num_pairs):\n",
        "        # Randomly sample two indices\n",
        "        idx1, idx2 = np.random.choice(len(dataset), 2, replace=False)\n",
        "        img1, label1 = dataset[idx1]\n",
        "        img2, label2 = dataset[idx2]\n",
        "\n",
        "        # Check if they are the same class or not\n",
        "        same_class = (label1 == label2)\n",
        "\n",
        "        # Resize and prepare pairs (you can apply the transformation here if necessary)\n",
        "        img1_resized = tf.image.resize(img1, image_size)\n",
        "        img2_resized = tf.image.resize(img2, image_size)\n",
        "\n",
        "        # Append the pair and the label\n",
        "        pairs.append([img1_resized, img2_resized])\n",
        "        labels.append(1 if same_class else 0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)"
      ],
      "metadata": {
        "id": "5SBPVIB7dttq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs, train_labels = generate_pairs(train_dataset, num_pairs=1000)"
      ],
      "metadata": {
        "id": "KdO3IOWIdv7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "collab keeps crashing here after using all availble ram"
      ],
      "metadata": {
        "id": "ysNQtIIwekyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_siamese_dataset(dataset, num_pairs):\n",
        "    pairs, labels = generate_pairs(dataset, num_pairs)\n",
        "\n",
        "    dataset_pairs = tf.data.Dataset.from_tensor_slices((pairs, labels))\n",
        "    dataset_pairs = dataset_pairs.batch(batch_size=128)\n",
        "\n",
        "    return dataset_pairs"
      ],
      "metadata": {
        "id": "dLToCf9UdwUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_siamese_dataset(train_dataset, num_pairs=1000)\n",
        "val_dataset = create_siamese_dataset(val_dataset, num_pairs=200)"
      ],
      "metadata": {
        "id": "Lhp9BnqKd1sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "G3ba5PT_V50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_subset, val_subset, epochs=10)"
      ],
      "metadata": {
        "id": "viuLa58uTOr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Siemese Network"
      ],
      "metadata": {
        "id": "qPX1w8y7esV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FusxkSbDerh0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_siamese_model(input_shape):\n",
        "    # Define the base model (shared for both inputs)\n",
        "    input = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Example of a simple convolutional base\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(input)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "\n",
        "    # Create a model with the defined architecture\n",
        "    base_model = Model(inputs=input, outputs=x)\n",
        "\n",
        "    return base_model\n",
        "\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    \"\"\"\n",
        "    Contrastive loss function to compare the similarity.\n",
        "    Args:\n",
        "        y_true: The true labels (0 or 1).\n",
        "        y_pred: The distance between the feature vectors.\n",
        "        margin: A margin value for dissimilar pairs.\n",
        "    Returns:\n",
        "        Loss value.\n",
        "    \"\"\"\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.maximum(margin - y_pred, 0)\n",
        "    loss = 0.5 * (y_true * square_pred + (1 - y_true) * tf.square(margin_square))\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def create_siamese_network(input_shape):\n",
        "    # Define input layers for the two inputs of the Siamese network\n",
        "    input_a = layers.Input(shape=input_shape)\n",
        "    input_b = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Create the base model that will be shared\n",
        "    base_model = create_siamese_model(input_shape)\n",
        "\n",
        "    # Get the feature embeddings for both inputs\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "    # Compute the L1 distance between the two embeddings\n",
        "    distance = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([processed_a, processed_b])\n",
        "\n",
        "    # Define the output layer to predict similarity\n",
        "    output = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
        "\n",
        "    # Create the full model\n",
        "    siamese_model = Model(inputs=[input_a, input_b], outputs=output)\n",
        "\n",
        "    # Compile the model with a loss function and optimizer\n",
        "    siamese_model.compile(optimizer=Adam(learning_rate=0.0001), loss=contrastive_loss, metrics=[\"accuracy\"])\n",
        "\n",
        "    return siamese_model\n"
      ],
      "metadata": {
        "id": "Jtti1LDxergq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, we'll use MNIST dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Pre-process data (resize and normalize)\n",
        "X_train = np.expand_dims(X_train, -1)  # Add channel dimension (grayscale)\n",
        "X_test = np.expand_dims(X_test, -1)\n",
        "X_train = X_train.astype(\"float32\") / 255\n",
        "X_test = X_test.astype(\"float32\") / 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRp0zqkEe0JI",
        "outputId": "23405715-8009-4d9d-f342-8b8dfce0f2bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair up images and assign labels (1 if they are the same digit, 0 otherwise)\n",
        "def create_pairs(X, y):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    digit_indices = [np.where(y == i)[0] for i in range(10)]\n",
        "    for i in range(len(X)):\n",
        "        current_image = X[i]\n",
        "        current_label = y[i]\n",
        "\n",
        "        # Positive pair: same digit\n",
        "        same_class_idx = np.random.choice(digit_indices[current_label])\n",
        "        pairs.append([current_image, X[same_class_idx]])\n",
        "        labels.append(1)\n",
        "\n",
        "        # Negative pair: different digit\n",
        "        diff_class_idx = np.random.choice(np.delete(np.arange(10), current_label))\n",
        "        diff_class_image = np.random.choice(digit_indices[diff_class_idx])\n",
        "        pairs.append([current_image, X[diff_class_image]])\n",
        "        labels.append(0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)"
      ],
      "metadata": {
        "id": "BG39wD-Oe1pY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training pairs and labels\n",
        "train_pairs, train_labels = create_pairs(X_train, y_train)\n",
        "test_pairs, test_labels = create_pairs(X_test, y_test)"
      ],
      "metadata": {
        "id": "WxrYWRs0e3_R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the pairs for Siamese model input\n",
        "train_pairs = np.array(train_pairs)\n",
        "test_pairs = np.array(test_pairs)"
      ],
      "metadata": {
        "id": "XUXJyFx6fOXv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape (e.g., 28x28 for MNIST)\n",
        "input_shape = (28, 28, 1)  # For MNIST dataset\n",
        "\n",
        "# Create the Siamese network\n",
        "siamese_model = create_siamese_network(input_shape)"
      ],
      "metadata": {
        "id": "MjdDxYSrfVbx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "siamese_model.fit([train_pairs[:, 0], train_pairs[:, 1]], train_labels, batch_size=64, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vxq6WyofaYg",
        "outputId": "32317d0a-1f5d-4034-ee0f-6b187b2ea68e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.1810 - loss: 0.0614\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.0376 - loss: 0.0153\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.0250 - loss: 0.0102\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.0177 - loss: 0.0074\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.0129 - loss: 0.0055\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.0095 - loss: 0.0043\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.0074 - loss: 0.0034\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.0056 - loss: 0.0027\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.0046 - loss: 0.0022\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.0033 - loss: 0.0018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79e192000ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = siamese_model.evaluate([test_pairs[:, 0], test_pairs[:, 1]], test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jbMwojfbXo",
        "outputId": "9f7b98ba-4659-43bb-b6a7-8cb62bd11256"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.0238 - loss: 0.0092\n",
            "Test accuracy: 0.0215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WodKIBcehHbN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = siamese_model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
        "\n",
        "# Show some predictions\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(5):\n",
        "    ax = plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(test_pairs[i, 0].reshape(28, 28), cmap=\"gray\")\n",
        "    plt.title(f\"Pred: {predictions[i]:.2f}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Cv0jf_i1ff8n",
        "outputId": "1c498beb-bd6b-4fd2-d9cf-5f8183b2ebf7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to numpy.ndarray.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-03f084ff88df>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pred: {predictions[i]:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss and accuracy\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TvZ7wV1ohXKr",
        "outputId": "24da48e3-c65e-40fc-b890-2c1f65aca48c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2df9518eb028>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}